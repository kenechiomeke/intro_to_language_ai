{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenechiomeke/intro_to_language_ai/blob/main/getting_started_tokenisers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization_intro"
      },
      "source": [
        "# üî§‚û°Ô∏èüî¢ Tokenization Playground: Breaking Language Into Pieces\n",
        "\n",
        "Welcome to your interactive tokenization laboratory! This notebook will help you understand how different tokenization strategies work by letting you experiment with them hands-on.\n",
        "\n",
        "## üìö What You'll Learn:\n",
        "- How to implement different tokenization methods\n",
        "- When to use each strategy\n",
        "- The trade-offs between different approaches\n",
        "- How tokenization affects downstream NLP tasks\n",
        "\n",
        "## üöÄ Getting Started:\n",
        "1. Run each cell in order (Shift + Enter)\n",
        "2. Experiment with different text inputs\n",
        "3. Compare results across tokenization methods\n",
        "4. Try your own examples!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## üì¶ Step 1: Install Required Libraries\n",
        "\n",
        "First, let's install all the libraries we'll need. This might take a minute!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_libraries",
        "outputId": "32fd86ce-00f4-4f38-b0d1-3046bb314c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ All libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers tokenizers sentencepiece nltk spacy pandas matplotlib seaborn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ All libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_section"
      },
      "source": [
        "## üìö Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "import_libraries",
        "outputId": "ac1f20f4-2ff0-4e7c-ac0a-63f9617ac562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_text_section"
      },
      "source": [
        "## üìù Step 3: Prepare Sample Text\n",
        "\n",
        "Let's create some diverse sample text that will showcase the differences between tokenization methods. We'll include:\n",
        "- Regular sentences\n",
        "- Technical terms\n",
        "- Social media style text\n",
        "- Multilingual content\n",
        "- Numbers and special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sample_text",
        "outputId": "245b8105-3650-4cbf-94a8-c48cd35626f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Sample Texts for Tokenization:\n",
            "==================================================\n",
            "\n",
            "üîπ SIMPLE:\n",
            "Hello world! How are you today?\n",
            "\n",
            "üîπ TECHNICAL:\n",
            "ChatGPT uses transformer architecture with self-attention mechanisms. It's pre-trained on massive datasets.\n",
            "\n",
            "üîπ SOCIAL_MEDIA:\n",
            "OMG! Just saw the most amazing sunset!!! #beautiful #nofilter Can't believe it's already 2024... Time flies!\n",
            "\n",
            "üîπ COMPLEX_WORDS:\n",
            "The unhappiness of the disconnected users was unprecedented. They were extraordinarily disappointed.\n",
            "\n",
            "üîπ MULTILINGUAL:\n",
            "Hello! Bonjour! Hola! The weather is nice today. Il fait beau aujourd'hui.\n",
            "\n",
            "üîπ NUMBERS_CODE:\n",
            "The function calculate_loss() returned 0.0045 after 1000 iterations. Error code: HTTP-404.\n",
            "\n",
            "üîπ MISSPELLINGS:\n",
            "I definatly think your right about this. Recieve my sincere apoligies for the delay.\n",
            "\n",
            "==================================================\n",
            "üí° Want to try your own text? Add it below and re-run the tokenization cells!\n"
          ]
        }
      ],
      "source": [
        "# Sample texts to demonstrate different tokenization challenges\n",
        "sample_texts = {\n",
        "    \"simple\": \"Hello world! How are you today?\",\n",
        "\n",
        "    \"technical\": \"ChatGPT uses transformer architecture with self-attention mechanisms. It's pre-trained on massive datasets.\",\n",
        "\n",
        "    \"social_media\": \"OMG! Just saw the most amazing sunset!!! #beautiful #nofilter Can't believe it's already 2024... Time flies!\",\n",
        "\n",
        "    \"complex_words\": \"The unhappiness of the disconnected users was unprecedented. They were extraordinarily disappointed.\",\n",
        "\n",
        "    \"multilingual\": \"Hello! Bonjour! Hola! The weather is nice today. Il fait beau aujourd'hui.\",\n",
        "\n",
        "    \"numbers_code\": \"The function calculate_loss() returned 0.0045 after 1000 iterations. Error code: HTTP-404.\",\n",
        "\n",
        "    \"misspellings\": \"I definatly think your right about this. Recieve my sincere apoligies for the delay.\"\n",
        "}\n",
        "\n",
        "# Display sample texts\n",
        "print(\"üìù Sample Texts for Tokenization:\")\n",
        "print(\"=\" * 50)\n",
        "for name, text in sample_texts.items():\n",
        "    print(f\"\\nüîπ {name.upper()}:\")\n",
        "    print(f\"{text}\")\n",
        "\n",
        "# Let users add their own text\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üí° Want to try your own text? Add it below and re-run the tokenization cells!\")\n",
        "sample_texts[\"custom\"] = \"Add your own text here to see how different tokenizers handle it!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "word_tokenization_section"
      },
      "source": [
        "## üî§ Method 1: Word-Level Tokenization\n",
        "\n",
        "The simplest approach: split text by spaces and punctuation. This is intuitive but has limitations with compound words and out-of-vocabulary terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "word_tokenization"
      },
      "outputs": [],
      "source": [
        "def word_tokenize_simple(text):\n",
        "    \"\"\"Simple word tokenization by splitting on whitespace and punctuation\"\"\"\n",
        "    # Remove extra whitespace and split\n",
        "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def word_tokenize_nltk(text):\n",
        "    \"\"\"NLTK's word tokenization (more sophisticated)\"\"\"\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "def word_tokenize_spacy(text):\n",
        "    \"\"\"spaCy's word tokenization (handles linguistics better)\"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.text.lower() for token in doc]\n",
        "\n",
        "# Test word tokenization methods\n",
        "print(\"üî§ WORD-LEVEL TOKENIZATION COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_text = sample_texts[\"technical\"]\n",
        "print(f\"Original text: {test_text}\")\n",
        "print()\n",
        "\n",
        "methods = {\n",
        "    \"Simple (Regex)\": word_tokenize_simple,\n",
        "    \"NLTK\": word_tokenize_nltk,\n",
        "    \"spaCy\": word_tokenize_spacy\n",
        "}\n",
        "\n",
        "for method_name, method_func in methods.items():\n",
        "    tokens = method_func(test_text)\n",
        "    print(f\"üìå {method_name}:\")\n",
        "    print(f\"  Tokens: {tokens}\")\n",
        "    print(f\"  Count: {len(tokens)} tokens\")\n",
        "    print()\n",
        "\n",
        "print(\"üí° Notice how different methods handle contractions and punctuation differently!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "char_tokenization_section"
      },
      "source": [
        "## üî° Method 2: Character-Level Tokenization\n",
        "\n",
        "Split text into individual characters. This handles any text but loses word-level meaning and creates very long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "char_tokenization"
      },
      "outputs": [],
      "source": [
        "def char_tokenize(text):\n",
        "    \"\"\"Character-level tokenization\"\"\"\n",
        "    return list(text)\n",
        "\n",
        "def char_tokenize_no_spaces(text):\n",
        "    \"\"\"Character-level tokenization without spaces\"\"\"\n",
        "    return [char for char in text if char != ' ']\n",
        "\n",
        "# Test character tokenization\n",
        "print(\"üî° CHARACTER-LEVEL TOKENIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_text = sample_texts[\"simple\"]\n",
        "print(f\"Original text: '{test_text}'\")\n",
        "print()\n",
        "\n",
        "# With spaces\n",
        "char_tokens = char_tokenize(test_text)\n",
        "print(f\"üìå With spaces:\")\n",
        "print(f\"  Tokens: {char_tokens}\")\n",
        "print(f\"  Count: {len(char_tokens)} tokens\")\n",
        "print()\n",
        "\n",
        "# Without spaces\n",
        "char_tokens_no_space = char_tokenize_no_spaces(test_text)\n",
        "print(f\"üìå Without spaces:\")\n",
        "print(f\"  Tokens: {char_tokens_no_space}\")\n",
        "print(f\"  Count: {len(char_tokens_no_space)} tokens\")\n",
        "print()\n",
        "\n",
        "# Show vocabulary size\n",
        "unique_chars = set(char_tokens)\n",
        "print(f\"üí° Unique characters (vocabulary): {sorted(unique_chars)}\")\n",
        "print(f\"  Vocabulary size: {len(unique_chars)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpe_tokenization_section"
      },
      "source": [
        "## üß© Method 3: Byte Pair Encoding (BPE)\n",
        "\n",
        "The breakthrough subword method! BPE learns the most frequent character pairs and merges them iteratively. This balances vocabulary size with meaningful chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpe_tokenization"
      },
      "outputs": [],
      "source": [
        "# Initialize GPT-2 tokenizer for BPE example\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Create a simple BPE tokenizer (for illustrative purposes, not for production)\n",
        "def create_bpe_tokenizer(texts, vocab_size=1000):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Train a small BPE tokenizer on our sample texts\n",
        "corpus = list(sample_texts.values())\n",
        "custom_bpe_tokenizer = create_bpe_tokenizer(corpus, vocab_size=200)\n",
        "\n",
        "# Test BPE tokenization\n",
        "print(\"üß© BYTE PAIR ENCODING (BPE) TOKENIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_text = sample_texts[\"social_media\"]\n",
        "print(f\"Original text: '{test_text}'\")\n",
        "print()\n",
        "\n",
        "# Using pre-trained GPT-2 tokenizer\n",
        "gpt2_tokens = gpt2_tokenizer.tokenize(test_text)\n",
        "print(f\"üìå GPT-2 BPE Tokenizer:\")\n",
        "print(f\"  Tokens: {gpt2_tokens}\")\n",
        "print(f\"  Count: {len(gpt2_tokens)} tokens\")\n",
        "print()\n",
        "\n",
        "# Using our custom trained BPE tokenizer\n",
        "custom_bpe_encoding = custom_bpe_tokenizer.encode(test_text)\n",
        "print(f\"üìå Custom BPE Tokenizer (vocab_size=200):\")\n",
        "print(f\"  Tokens: {custom_bpe_encoding.tokens}\")\n",
        "print(f\"  Count: {len(custom_bpe_encoding.tokens)} tokens\")\n",
        "print(f\"  IDs: {custom_bpe_encoding.ids}\")\n",
        "print()\n",
        "\n",
        "test_text_oov = \"supercalifragilisticexpialidocious\"\n",
        "print(f\"Original OOV text: '{test_text_oov}'\")\n",
        "print(f\"GPT-2 OOV tokens: {gpt2_tokenizer.tokenize(test_text_oov)}\")\n",
        "print(\"üí° Notice how BPE breaks down unknown words into known subword units!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sentencepiece_section"
      },
      "source": [
        "## üåç Method 4: SentencePiece (for multilingual and robust tokenization)\n",
        "\n",
        "SentencePiece is a language-agnostic subword tokenizer that can be trained on raw text without pre-tokenization. It's great for multilingual models and consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sentencepiece_tokenization"
      },
      "outputs": [],
      "source": [
        "# Prepare data for SentencePiece training (requires a text file)\n",
        "corpus_file = \"corpus.txt\"\n",
        "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for text in sample_texts.values():\n",
        "        f.write(text + \"\\n\")\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f'--input={corpus_file} --model_prefix=m --vocab_size=200 --model_type=unigram'\n",
        ")\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp_tokenizer = spm.SentencePieceProcessor()\n",
        "sp_tokenizer.load(\"m.model\")\n",
        "\n",
        "print(\"üåç SENTENCEPIECE TOKENIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_text = sample_texts[\"multilingual\"]\n",
        "print(f\"Original text: '{test_text}'\")\n",
        "print()\n",
        "\n",
        "sp_tokens = sp_tokenizer.encode_as_pieces(test_text)\n",
        "print(f\"üìå SentencePiece Tokens:\")\n",
        "print(f\"  Tokens: {sp_tokens}\")\n",
        "print(f\"  Count: {len(sp_tokens)} tokens\")\n",
        "print()\n",
        "\n",
        "test_text_korean = \"ÏïàÎÖïÌïòÏÑ∏Ïöî ÏÑ∏ÏÉÅ\"\n",
        "print(f\"Original Korean text: '{test_text_korean}'\")\n",
        "sp_tokens_korean = sp_tokenizer.encode_as_pieces(test_text_korean)\n",
        "print(f\"üìå SentencePiece Korean Tokens: {sp_tokens_korean}\")\n",
        "print(\"üí° SentencePiece handles diverse languages gracefully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "token_scale_section"
      },
      "source": [
        "## üìä Token Scale and LLMs: What does '1 Trillion Tokens' Mean?\n",
        "\n",
        "You often hear about Large Language Models (LLMs) being trained on vast amounts of data, measured in 'tokens'. Let's explore what that means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "token_scale_analysis"
      },
      "outputs": [],
      "source": [
        "def analyze_token_scale():\n",
        "    print(\"üìä ANALYZING TOKEN SCALE AND LLMs\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    llm_sizes = {\n",
        "        \"Small LLM (e.g., Early GPT)\": {\"tokens\": 1 * (10**9), \"description\": \"1 Billion tokens\"},\n",
        "        \"Medium LLM (e.g., GPT-3 Small)\": {\"tokens\": 10 * (10**9), \"description\": \"10 Billion tokens\"},\n",
        "        \"Large LLM (e.g., GPT-3)\": {\"tokens\": 300 * (10**9), \"description\": \"300 Billion tokens\"},\n",
        "        \"Very Large LLM (e.g., PaLM, GPT-4)\": {\"tokens\": 1 * (10**12), \"description\": \"1 Trillion tokens\"}\n",
        "    }\n",
        "\n",
        "    avg_tokens_per_word = 1.3 # Common ratio for subword tokenizers\n",
        "    avg_words_per_page = 500\n",
        "    avg_pages_per_book = 300\n",
        "    avg_words_per_tweet = 20\n",
        "\n",
        "    print(\"\\nApproximate Equivalents:\")\n",
        "    for name, data in llm_sizes.items():\n",
        "        tokens = data['tokens']\n",
        "        description = data['description']\n",
        "\n",
        "        words = tokens / avg_tokens_per_word\n",
        "        books = words / (avg_words_per_page * avg_pages_per_book)\n",
        "        tweets = words / avg_words_per_tweet\n",
        "\n",
        "        print(f\"\\nüöÄ {name} ({description}):\")\n",
        "        print(f\"  Approx. words: {words:,.0f}\")\n",
        "        print(f\"  Equivalent to approx. {books:,.0f} books\")\n",
        "        print(f\"  Equivalent to approx. {tweets:,.0f} tweets\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üí° Why does more tokens matter?\")\n",
        "    print(\"  - **Wider Exposure:** Model sees more diverse language, facts, and styles.\")\n",
        "    print(\"  - **Deeper Understanding:** Learns more complex patterns and relationships.\")\n",
        "    print(\"  - **Better Generalization:** Performs better on unseen data and tasks.\")\n",
        "    print(\"  - **Emergent Capabilities:** Larger scale can unlock new abilities (e.g., reasoning).\")\n",
        "\n",
        "    print(\"\\n  Is an LLM trained on 1T better than one trained on 1B?\")\n",
        "    print(\"  **Generally, YES!** 1 Trillion tokens is 1000 times more data than 1 Billion tokens. \\n  This massive difference in training data typically leads to vastly superior performance, \\n  more robust understanding, and greater capabilities in an LLM.\")\n",
        "    print(\"  However, the *quality* and *diversity* of tokens also matter, not just the quantity.\")\n",
        "\n",
        "token_stats = analyze_token_scale()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hands_on_section"
      },
      "source": [
        "## üõ†Ô∏è Hands-On Experimentation\n",
        "\n",
        "Now it's your turn! Try different texts and see how tokenization affects the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hands_on_experiment"
      },
      "outputs": [],
      "source": [
        "# Interactive experimentation function\n",
        "def tokenize_and_analyze(text, show_details=True):\n",
        "    \"\"\"Tokenize text with multiple methods and show detailed analysis\"\"\"\n",
        "\n",
        "    print(f\"üîç ANALYZING: '{text}'\")\n",
        "    print(\"=\" * min(60, len(text) + 20))\n",
        "    print(f\"üìè Text length: {len(text)} characters\")\n",
        "    print(f\"üìù Word count: {len(text.split())} words\")\n",
        "    print()\n",
        "\n",
        "    # Tokenize with different methods\n",
        "    methods = {\n",
        "        \"Word (NLTK)\": word_tokenize_nltk(text),\n",
        "        \"Character\": char_tokenize(text),\n",
        "        \"GPT-2 BPE\": gpt2_tokenizer.tokenize(text)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for method_name, tokens in methods.items():\n",
        "        results[method_name] = {\n",
        "            'count': len(tokens),\n",
        "            'tokens': tokens,\n",
        "            'ratio': len(tokens) / len(text) if len(text) > 0 else 0,\n",
        "            'compression': len(text) / len(tokens) if tokens else 0\n",
        "        }\n",
        "\n",
        "        print(f\"üìå {method_name}:\")\n",
        "        print(f\"  Tokens: {len(tokens)}\")\n",
        "        print(f\"  Ratio: {len(tokens)/len(text):.3f} tokens/char\" if len(text) > 0 else \"  Ratio: N/A\")\n",
        "        print(f\"  Compression: {len(text)/len(tokens):.1f}x\" if tokens else \"  Compression: N/A\")\n",
        "\n",
        "        if show_details and len(tokens) <= 20:\n",
        "            print(f\"  All tokens: {tokens}\")\n",
        "        elif show_details:\n",
        "            print(f\"  First 10: {tokens[:10]}\")\n",
        "            print(f\"  Last 10: {tokens[-10:]}\")\n",
        "        print()\n",
        "\n",
        "    # Vocabulary analysis for longer texts\n",
        "    if len(text) > 50:\n",
        "        word_tokens = methods[\"Word (NLTK)\"]\n",
        "        unique_words = set(word_tokens)\n",
        "        word_freq = Counter(word_tokens)\n",
        "\n",
        "        print(\"üìä VOCABULARY ANALYSIS:\")\n",
        "        print(f\"  Unique words: {len(unique_words)}\")\n",
        "        print(f\"  Vocabulary richness: {len(unique_words)/len(word_tokens):.3f}\")\n",
        "        print(f\"  Most common: {word_freq.most_common(5)}\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example experiments\n",
        "print(\"üõ†Ô∏è HANDS-ON TOKENIZATION EXPERIMENTS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Try these examples or add your own text below!\\n\")\n",
        "\n",
        "# Experiment 1: Technical jargon\n",
        "experiment_texts = [\n",
        "    \"The transformer architecture uses self-attention mechanisms.\",\n",
        "    \"OMG! This is sooooo cool!!!\",\n",
        "    \"Machine learning enables computers to learn without explicit programming.\",\n",
        "    \"antidisestablishmentarianism pseudopseudohypoparathyroidism\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(experiment_texts, 1):\n",
        "    print(f\"\\nüß™ EXPERIMENT {i}:\")\n",
        "    tokenize_and_analyze(text, show_details=True)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nüí° TRY YOUR OWN TEXT:\")\n",
        "print(\"Replace the text below and run the cell to see how different methods tokenize it!\")\n",
        "\n",
        "# User's custom text (they can modify this)\n",
        "your_text = \"Replace this with any text you want to analyze!\"\n",
        "print(f\"\\nüéØ YOUR CUSTOM TEXT ANALYSIS:\")\n",
        "tokenize_and_analyze(your_text, show_details=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "best_practices_section"
      },
      "source": [
        "## üéØ When to Use Which Tokenization Method?\n",
        "\n",
        "Based on our experiments, here's a practical guide for choosing tokenization strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_practices"
      },
      "outputs": [],
      "source": [
        "def tokenization_decision_guide():\n",
        "    \"\"\"Interactive guide to help choose the right tokenization method\"\"\"\n",
        "\n",
        "    print(\"üéØ TOKENIZATION DECISION GUIDE\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    scenarios = {\n",
        "        \"üìö Clean Text Analysis\": {\n",
        "            \"description\": \"News articles, books, formal documents\",\n",
        "            \"recommendation\": \"Word-level tokenization (NLTK/spaCy)\",\n",
        "            \"why\": \"Text is well-formed, vocabulary is manageable\",\n",
        "            \"pros\": [\"Human-readable\", \"Preserves word meaning\", \"Fast processing\"],\n",
        "            \"cons\": [\"Struggles with OOV words\", \"Large vocabulary\", \"No subword info\"]\n",
        "        },\n",
        "\n",
        "        \"üåç Multilingual Applications\": {\n",
        "            \"description\": \"Apps supporting multiple languages\",\n",
        "            \"recommendation\": \"SentencePiece or multilingual BERT tokenizer\",\n",
        "            \"why\": \"Language-agnostic, handles different scripts\",\n",
        "            \"pros\": [\"Works across languages\", \"Consistent handling\", \"Space-aware\"],\n",
        "            \"cons\": [\"More complex setup\", \"Larger models\", \"Training required\"]\n",
        "        },\n",
        "\n",
        "        \"ü§ñ Large Language Models\": {\n",
        "            \"description\": \"Training or fine-tuning LLMs\",\n",
        "            \"recommendation\": \"BPE (GPT-style) or SentencePiece\",\n",
        "            \"why\": \"Balances vocabulary size with semantic meaning\",\n",
        "            \"pros\": [\"Efficient encoding\", \"Handles rare words\", \"Proven at scale\"],\n",
        "            \"cons\": [\"Less interpretable\", \"Training overhead\", \"Hyperparameter tuning\"]\n",
        "        },\n",
        "\n",
        "        \"üì± Social Media/Noisy Text\": {\n",
        "            \"description\": \"Tweets, comments, user-generated content\",\n",
        "            \"recommendation\": \"Robust BPE or character-level for extreme cases\",\n",
        "            \"why\": \"Handles misspellings, slang, and novel expressions\",\n",
        "            \"pros\": [\"Robust to noise\", \"Handles creativity\", \"No OOV issues\"],\n",
        "            \"cons\": [\"May lose word boundaries\", \"Longer sequences\", \"Complex preprocessing\"]\n",
        "        },\n",
        "\n",
        "        \"üî¨ Research/Prototyping\": {\n",
        "            \"description\": \"Quick experiments and proof of concepts\",\n",
        "            \"recommendation\": \"Pre-trained tokenizers (GPT-2, BERT)\",\n",
        "            \"why\": \"Ready to use, well-tested, community support\",\n",
        "            \"pros\": [\"No training needed\", \"Proven performance\", \"Easy integration\"],\n",
        "            \"cons\": [\"May not fit domain\", \"Fixed vocabulary\", \"Less control\"]\n",
        "        },\n",
        "\n",
        "        \"‚ö° Real-time Applications\": {\n",
        "            \"description\": \"Chatbots, live translation, streaming\",\n",
        "            \"recommendation\": \"Fast word-level or optimized BPE\",\n",
        "            \"why\": \"Speed is critical, latency matters\",\n",
        "            \"pros\": [\"Fast processing\", \"Low latency\", \"Predictable performance\"],\n",
        "            \"cons\": [\"May sacrifice accuracy\", \"Limited handling of edge cases\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for scenario, details in scenarios.items():\n",
        "        print(f\"\\n{scenario}\")\n",
        "        print(f\"üìù Use case: {details['description']}\")\n",
        "        print(f\"üéØ Recommended: {details['recommendation']}\")\n",
        "        print(f\"üí° Why: {details['why']}\")\n",
        "        print(f\"‚úÖ Pros: {', '.join(details['pros'])}\")\n",
        "        print(f\"‚ùå Cons: {', '.join(details['cons'])}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üöÄ QUICK DECISION FLOWCHART:\")\n",
        "    print(\"\"\"\n",
        "    ‚îå‚îÄ Multilingual? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí SentencePiece\n",
        "    ‚îÇ\n",
        "    ‚îú‚îÄ Large scale LLM? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí BPE (GPT-2 style)\n",
        "    ‚îÇ\n",
        "    ‚îú‚îÄ Clean, formal text? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí Word-level (NLTK)\n",
        "    ‚îÇ\n",
        "    ‚îú‚îÄ Noisy/social media? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí Robust BPE\n",
        "    ‚îÇ\n",
        "    ‚îú‚îÄ Quick prototype? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí Pre-trained tokenizer\n",
        "    ‚îÇ\n",
        "    ‚îî‚îÄ Real-time critical? ‚îÄ‚îÄ‚Üí YES ‚îÄ‚îÄ‚Üí Fast word-level\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\nüîß IMPLEMENTATION TIPS:\")\n",
        "    tips = [\n",
        "    \"Always evaluate tokenization on your specific domain\",\n",
        "    \"Consider vocabulary size vs. sequence length trade-offs\",\n",
        "    \"Test with edge cases (URLs, hashtags, code, etc.)\",\n",
        "    \"Monitor out-of-vocabulary rates during development\",\n",
        "    \"Use subword regularization for robustness (if available)\",\n",
        "    \"Document your tokenization choices for reproducibility\",\n",
        "    \"Consider computational constraints in production\"\n",
        "    ]\n",
        "\n",
        "    for i, tip in enumerate(tips, 1):\n",
        "    print(f\"  {i}. {tip}\")\n",
        "\n",
        "tokenization_decision_guide()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance_section"
      },
      "source": [
        "## ‚ö° Performance and Efficiency Analysis\n",
        "\n",
        "Let's measure the speed and memory efficiency of different tokenization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_analysis"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "def measure_tokenization_performance():\n",
        "    \"\"\"Measure speed and efficiency of different tokenization methods\"\"\"\n",
        "\n",
        "    # Create test text of different sizes\n",
        "    base_text = \" \".join(sample_texts.values())\n",
        "    test_texts = {\n",
        "        \"Small (1KB)\": base_text[:1000],\n",
        "        \"Medium (10KB)\": (base_text * 10)[:10000],\n",
        "        \"Large (100KB)\": (base_text * 100)[:100000]\n",
        "    }\n",
        "\n",
        "    methods = {\n",
        "        \"Word (NLTK)\": lambda x: word_tokenize_nltk(x),\n",
        "        \"Character\": lambda x: char_tokenize(x),\n",
        "        \"GPT-2 BPE\": lambda x: gpt2_tokenizer.tokenize(x)\n",
        "    }\n",
        "\n",
        "    print(\"‚ö° TOKENIZATION PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for text_size, text in test_texts.items():\n",
        "        print(f\"\\nüìè Testing with {text_size} text ({len(text):,} characters)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for method_name, method_func in methods.items():\n",
        "            # Measure time\n",
        "            start_time = time.time()\n",
        "            tokens = method_func(text)\n",
        "            end_time = time.time()\n",
        "\n",
        "            duration = end_time - start_time\n",
        "            chars_per_second = len(text) / duration if duration > 0 else float('inf')\n",
        "            tokens_per_second = len(tokens) / duration if duration > 0 else float('inf')\n",
        "\n",
        "            # Memory estimation (rough)\n",
        "            token_memory = sys.getsizeof(tokens)\n",
        "\n",
        "            print(f\"{method_name:12} | {duration*1000:6.1f}ms | {chars_per_second/1000:6.1f}K chars/s | {len(tokens):5d} tokens | {token_memory/1024:.1f}KB\")\n",
        "\n",
        "            results.append({\n",
        "                'Text Size': text_size,\n",
        "                'Method': method_name,\n",
        "                'Duration (ms)': duration * 1000,\n",
        "                'Chars/sec': chars_per_second,\n",
        "                'Tokens': len(tokens),\n",
        "                'Memory (KB)': token_memory / 1024\n",
        "            })\n",
        "\n",
        "    # Performance summary\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"üìä PERFORMANCE SUMMARY:\")\n",
        "\n",
        "    perf_df = pd.DataFrame(results)\n",
        "\n",
        "    # Average performance by method\n",
        "    avg_perf = perf_df.groupby('Method').agg({\n",
        "        'Duration (ms)': 'mean',\n",
        "        'Chars/sec': 'mean',\n",
        "        'Memory (KB)': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    print(\"\\nüìà Average Performance by Method:\")\n",
        "    print(avg_perf)\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Speed comparison\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for method in perf_df['Method'].unique():\n",
        "        method_data = perf_df[perf_df['Method'] == method]\n",
        "        plt.plot(range(len(method_data)), method_data['Chars/sec'],\n",
        "                         marker='o', label=method)\n",
        "    plt.xlabel('Text Size Category')\n",
        "    plt.ylabel('Characters/Second')\n",
        "    plt.title('Processing Speed')\n",
        "    plt.legend()\n",
        "    plt.yscale('log')\n",
        "\n",
        "    # Token efficiency\n",
        "    plt.subplot(1, 3, 2)\n",
        "    pivot_tokens = perf_df.pivot(index='Text Size', columns='Method', values='Tokens')\n",
        "    pivot_tokens.plot(kind='bar', ax=plt.gca())\n",
        "    plt.title('Token Count by Method')\n",
        "    plt.ylabel('Number of Tokens')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Memory usage\n",
        "    plt.subplot(1, 3, 3)\n",
        "    for method in perf_df['Method'].unique():\n",
        "        method_data = perf_df[perf_df['Method'] == method]\n",
        "        plt.plot(range(len(method_data)), method_data['Memory (KB)'],\n",
        "                         marker='s', label=method)\n",
        "    plt.xlabel('Text Size Category')\n",
        "    plt.ylabel('Memory Usage (KB)')\n",
        "    plt.title('Memory Efficiency')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüéØ KEY PERFORMANCE INSIGHTS:\")\n",
        "    fastest_method = avg_perf['Chars/sec'].idxmax()\n",
        "    most_memory_efficient = avg_perf['Memory (KB)'].idxmin()\n",
        "\n",
        "    print(f\"‚ö° Fastest method: {fastest_method}\")\n",
        "    print(f\"üíæ Most memory efficient: {most_memory_efficient}\")\n",
        "    print(f\"üî§ Character tokenization creates {perf_df[perf_df['Method']=='Character']['Tokens'].iloc[0]/perf_df[perf_df['Method']=='Word (NLTK)']['Tokens'].iloc[0]:.1f}x more tokens than word tokenization\")\n",
        "\n",
        "measure_tokenization_performance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## üéì Summary and Next Steps\n",
        "\n",
        "Congratulations! You've now experienced hands-on how different tokenization strategies work and their trade-offs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary_next_steps"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def generate_learning_summary():\n",
        "    \"\"\"Generate a personalized learning summary\"\"\"\n",
        "\n",
        "    print(\"üéì TOKENIZATION MASTERY SUMMARY\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    print(\"\\n‚úÖ WHAT YOU'VE LEARNED:\")\n",
        "    learnings = [\n",
        "        \"How computers break human language into processable pieces\",\n",
        "        \"The evolution from word-level to sophisticated subword methods\",\n",
        "        \"Why BPE revolutionized NLP and enabled large language models\",\n",
        "        \"How to choose the right tokenization for your use case\",\n",
        "        \"The relationship between tokens and LLM training scale\",\n",
        "        \"Performance trade-offs between different approaches\"\n",
        "    ]\n",
        "\n",
        "    for i, learning in enumerate(learnings, 1):\n",
        "        print(f\"  {i}. {learning}\")\n",
        "\n",
        "    print(\"\\nüîß PRACTICAL SKILLS GAINED:\")\n",
        "    skills = [\n",
        "        \"Implementing word, character, and BPE tokenization\",\n",
        "        \"Using production tokenizers (GPT-2, SentencePiece)\",\n",
        "        \"Analyzing tokenization performance and efficiency\",\n",
        "        \"Debugging tokenization issues with real examples\",\n",
        "        \"Making informed decisions about tokenization strategies\"\n",
        "    ]\n",
        "\n",
        "    for i, skill in enumerate(skills, 1):\n",
        "        print(f\"  {i}. {skill}\")\n",
        "\n",
        "    print(\"\\nüöÄ NEXT STEPS IN YOUR NLP JOURNEY:\")\n",
        "    next_steps = [\n",
        "        \"üìö Word Embeddings: How tokens become meaningful vectors\",\n",
        "        \"üîç Attention Mechanisms: How models focus on relevant tokens\",\n",
        "        \"üèóÔ∏è Transformer Architecture: The building blocks of modern LLMs\",\n",
        "        \"‚öôÔ∏è Fine-tuning: Adapting pre-trained models to your domain\",\n",
        "        \"üéØ Evaluation: Measuring NLP model performance\",\n",
        "        \"üîß Production: Deploying NLP models at scale\"\n",
        "    ]\n",
        "\n",
        "    for step in next_steps:\n",
        "        print(f\"  ‚Ä¢ {step}\")\n",
        "\n",
        "    print(\"\\nüí° EXPERIMENT IDEAS:\")\n",
        "    experiments = [\n",
        "        \"Try tokenizing text in different languages\",\n",
        "        \"Compare tokenization of code vs natural language\",\n",
        "        \"Analyze how tokenization affects sentiment analysis\",\n",
        "        \"Experiment with custom BPE vocabulary sizes\",\n",
        "        \"Build a simple text classifier using different tokenizers\",\n",
        "        \"Measure tokenization impact on model performance\"\n",
        "    ]\n",
        "\n",
        "    for experiment in experiments:\n",
        "        print(f\"  üß™ {experiment}\")\n",
        "\n",
        "    print(\"\\nüìö RECOMMENDED RESOURCES:\")\n",
        "    resources = [\n",
        "        \"üîó Hugging Face Tokenizers Library Documentation\",\n",
        "        \"üìñ 'Natural Language Processing with Python' (NLTK Book)\",\n",
        "        \"üé• Andrej Karpathy's 'Let's build GPT' video series\",\n",
        "        \"üìÑ Original BPE paper: 'Neural Machine Translation of Rare Words'\",\n",
        "        \"üåê OpenAI's GPT-2 tokenizer implementation\",\n",
        "        \"üî¨ Google's SentencePiece research papers\"\n",
        "    ]\n",
        "\n",
        "    for resource in resources:\n",
        "        print(f\"  {resource}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 45)\n",
        "    print(\"üåü Remember: Great tokenization is the foundation of great NLP!\")\n",
        "    print(\"Keep experimenting, keep learning, and most importantly...\")\n",
        "    print(\"üöÄ Keep building amazing things with language AI!\")\n",
        "\n",
        "    # Generate a completion certificate\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üèÜ TOKENIZATION MASTERY CERTIFICATE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"This certifies that you have successfully completed\")\n",
        "    print(f\"the Interactive Tokenization Workshop and demonstrated\")\n",
        "    print(f\"understanding of fundamental NLP tokenization concepts.\")\n",
        "    print(f\"\")\n",
        "    print(f\"Date: {time.strftime('%Y-%m-%d')}\")\n",
        "    print(f\"Workshop: Breaking Language Into Pieces\")\n",
        "    print(f\"\")\n",
        "    print(f\"Skills Demonstrated:\")\n",
        "    print(f\"‚úì Word-level tokenization\")\n",
        "    print(f\"‚úì Character-level tokenization\")\n",
        "    print(f\"‚úì Byte Pair Encoding (BPE)\")\n",
        "    print(f\"‚úì Production tokenizer usage\")\n",
        "generate_learning_summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}